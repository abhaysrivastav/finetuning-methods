{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhaysrivastav/finetuning-methods/blob/main/Chapter3_Demo1_TransferLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfff6383",
      "metadata": {
        "id": "bfff6383"
      },
      "source": [
        "# Translation from English to Spanish using Flan-T5 and Helsinki-NLP/opus-100 Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9f128b9",
      "metadata": {
        "id": "e9f128b9"
      },
      "source": [
        "\n",
        "## Introduction\n",
        "In this notebook, we will use the Flan-T5 model to perform translation from English to Spanish. We will use the \"Helsinki-NLP/opus-100\" dataset from Hugging Face, specifically the en-es subset, to train and evaluate our translation model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7152441a",
      "metadata": {
        "id": "7152441a"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers tensorflow datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8885d52e",
      "metadata": {
        "id": "8885d52e"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e985d020",
      "metadata": {
        "id": "e985d020"
      },
      "source": [
        "## Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e3f06d1",
      "metadata": {
        "id": "5e3f06d1"
      },
      "outputs": [],
      "source": [
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the Helsinki-NLP/opus-100 dataset\n",
        "dataset = load_dataset('Helsinki-NLP/opus-100', 'en-es')\n",
        "print(dataset['train'][0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "065dd4f0",
      "metadata": {
        "id": "065dd4f0"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "130d92fb",
      "metadata": {
        "id": "130d92fb"
      },
      "outputs": [],
      "source": [
        "# Preprocess the dataset for input into the model\n",
        "def preprocess_data(examples):\n",
        "    inputs = [f'Translate from English to Spanish: {example[\"en\"]}' for example in examples['translation']]\n",
        "    targets = [example['es'] for example in examples['translation']]\n",
        "\n",
        "    # Tokenize inputs\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # Tokenize targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    # For decoder inputs\n",
        "    decoder_inputs = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"decoder_input_ids\"] = decoder_inputs[\"input_ids\"]\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "# Apply preprocessing to the dataset\n",
        "train_dataset = dataset['train'].select(range(30000)).map(preprocess_data, batched=True)\n",
        "test_dataset = dataset['test'].map(preprocess_data, batched=True)\n",
        "\n",
        "print(train_dataset[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train_dataset))\n",
        "print(type(test_dataset))"
      ],
      "metadata": {
        "id": "G5e8DiGEX9RD"
      },
      "id": "G5e8DiGEX9RD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "52fd5609",
      "metadata": {
        "id": "52fd5609"
      },
      "source": [
        "## Freezing the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E-zjSYjy0yLn",
      "metadata": {
        "id": "E-zjSYjy0yLn"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc180d66",
      "metadata": {
        "id": "dc180d66"
      },
      "outputs": [],
      "source": [
        "for param in model.shared.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Freeze the encoder\n",
        "for param in model.encoder.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Freeze the decoder\n",
        "for param in model.decoder.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sWh-GvlZ0zu_",
      "metadata": {
        "id": "sWh-GvlZ0zu_"
      },
      "outputs": [],
      "source": [
        "def params_info(model):\n",
        "    total_params = 0\n",
        "    trainable_params = 0\n",
        "    for param in model.parameters():\n",
        "        num = param.numel()\n",
        "        total_params += num\n",
        "        if param.requires_grad:\n",
        "            trainable_params += num\n",
        "    non_trainable_params = total_params - trainable_params\n",
        "\n",
        "    def size_in_mb(param_count):\n",
        "        return param_count * 4 / 1024 / 1024  # 4 bytes per param for float32\n",
        "\n",
        "    print(f\"Total params: {total_params:,} ({size_in_mb(total_params):.2f} MB)\")\n",
        "    print(f\"Trainable params: {trainable_params:,} ({size_in_mb(trainable_params):.2f} MB)\")\n",
        "    print(f\"Non-trainable params: {non_trainable_params:,} ({size_in_mb(non_trainable_params):.2f} MB)\")\n",
        "\n",
        "# Usage\n",
        "params_info(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab4d5c6c",
      "metadata": {
        "id": "ab4d5c6c"
      },
      "source": [
        "\n",
        "### Important Considerations in Transfer Learning\n",
        "\n",
        "1. **Freezing the LLM Layer:** In transfer learning, it's important to freeze the pre-trained language model layer to retain the knowledge it has already acquired and to avoid overfitting. This allows the model to leverage its pre-trained capabilities while focusing on learning the new task-specific nuances.\n",
        "\n",
        "2. **Loss Function with `from_logits=True`:** When fine-tuning language models from Hugging Face, it's crucial to use the loss function with `from_logits=True`. This is because these models do not apply softmax to their outputs, and using `from_logits=True` ensures that the loss is computed correctly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5791b03e",
      "metadata": {
        "id": "5791b03e"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "fIX9403Ibyrm"
      },
      "id": "fIX9403Ibyrm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8608ebea",
      "metadata": {
        "id": "8608ebea"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,      # should be Hugging Face Dataset, not tf.data.Dataset\n",
        "    eval_dataset=test_dataset,        # should be Hugging Face Dataset, not tf.data.Dataset\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "073993dd",
      "metadata": {
        "id": "073993dd"
      },
      "source": [
        "## Performing Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53933e6b",
      "metadata": {
        "id": "53933e6b"
      },
      "outputs": [],
      "source": [
        "metrics = trainer.evaluate()\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./finetuned-flan-t5\")\n",
        "tokenizer.save_pretrained(\"./finetuned-flan-t5\")"
      ],
      "metadata": {
        "id": "R8AuK6bnnf6t"
      },
      "id": "R8AuK6bnnf6t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Translate from English to Spanish: How are you?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Move input tensors to the same device as the model\n",
        "device = next(model.parameters()).device  # Get model device\n",
        "for k in inputs:\n",
        "    inputs[k] = inputs[k].to(device)\n",
        "\n",
        "outputs = model.generate(**inputs)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "vmj66we1nlTL"
      },
      "id": "vmj66we1nlTL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b25d96fa",
      "metadata": {
        "id": "b25d96fa"
      },
      "source": [
        "\n",
        "## Conclusion\n",
        "In this notebook, we used the Flan-T5 model to perform translation from English to Spanish using the Helsinki-NLP/opus-100 dataset. We preprocessed the dataset, fine-tuned the model while freezing the LLM layer, and performed translations. We manually validated the translations to assess the quality of the model's performance. The results demonstrate the effectiveness of the Flan-T5 model for translation tasks.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}