{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhaysrivastav/finetuning-methods/blob/main/Chapter2_Demo2_Prompting_FlanT5_ICL_FewShot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c0f4eb8",
      "metadata": {
        "id": "2c0f4eb8"
      },
      "source": [
        "# Enhanced Prompting with Flan-T5 using In-Context Learning and Few-Shot Pattern"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b876265b",
      "metadata": {
        "id": "b876265b"
      },
      "outputs": [],
      "source": [
        "!pip install transformers tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "338d0ee5",
      "metadata": {
        "id": "338d0ee5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b0ec416",
      "metadata": {
        "id": "2b0ec416"
      },
      "source": [
        "## Summarization with Few-Shot Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e1328aa",
      "metadata": {
        "id": "6e1328aa"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Here, we provide a few examples along with their summaries to help the model understand the pattern.\n",
        "\n",
        "# Few-shot examples\n",
        "few_shot_examples = [\n",
        "    \"Summarise: 'The quick brown fox jumps over the lazy dog. The dog was not amused by the fox's antics.' Summary: 'The fox jumped over the dog, who was not happy.'\",\n",
        "    \"Summarise: 'The rain in Spain stays mainly in the plain. It was a wet and rainy season in the Spanish plains.' Summary: 'It was a rainy season in the Spanish plains.'\"\n",
        "]\n",
        "\n",
        "# New prompt to summarize\n",
        "prompt = \"Summarise: 'Studies show that eating carrots helps improve vision. Carrots contain beta-carotene, a substance that the body converts into vitamin A, crucial for maintaining healthy eyesight.'\"\n",
        "\n",
        "# Combine examples and prompt\n",
        "combined_prompt = \"\\n\\n\".join(few_shot_examples + [prompt])\n",
        "\n",
        "# Generate inputs\n",
        "inputs = tokenizer(combined_prompt, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
        "\n",
        "# Generate outputs\n",
        "outputs = model.generate(inputs[\"input_ids\"], max_length=150, num_beams=5, early_stopping=True)\n",
        "\n",
        "# Decode and print the summary\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e9d081a",
      "metadata": {
        "id": "8e9d081a"
      },
      "source": [
        "## Translation with Few-Shot Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df94fbee",
      "metadata": {
        "id": "df94fbee"
      },
      "outputs": [],
      "source": [
        "# Here, we provide a few examples along with their translations to help the model understand the pattern.\n",
        "\n",
        "# Few-shot examples\n",
        "few_shot_examples = [\n",
        "    \"translate English to Spanish: 'The cat sits on the mat.' Translation: 'El gato se sienta en la alfombra.'\",\n",
        "    \"translate English to Spanish: 'The sun is shining brightly.' Translation: 'El sol brilla intensamente.'\"\n",
        "]\n",
        "\n",
        "# New prompt to translate\n",
        "translation_prompt = \"translate English to Spanish: 'Cheese is delicious.'\"\n",
        "\n",
        "# Combine examples and prompt\n",
        "combined_translation_prompt = \"\\n\\n\".join(few_shot_examples + [translation_prompt])\n",
        "\n",
        "# Prepare inputs and generate outputs\n",
        "translation_inputs = tokenizer(combined_translation_prompt, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
        "translation_outputs = model.generate(translation_inputs[\"input_ids\"], max_length=40, num_beams=5, early_stopping=True)\n",
        "\n",
        "# Decode and display the translation\n",
        "print(tokenizer.decode(translation_outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb551e91",
      "metadata": {
        "id": "fb551e91"
      },
      "source": [
        "## Q&A with In-Context Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acc290a8",
      "metadata": {
        "id": "acc290a8"
      },
      "outputs": [],
      "source": [
        "# Here, we provide a few examples along with their answers to help the model understand the pattern.\n",
        "\n",
        "# Few-shot examples\n",
        "few_shot_examples = [\n",
        "    \"The Great Wall of China is over 13,000 miles long. question: 'How long is the Great Wall of China?' answer: 'The Great Wall of China is over 13,000 miles long.'\",\n",
        "    \"The capital of France is Paris. question: 'What is the capital of France?' answer: 'The capital of France is Paris.'\"\n",
        "]\n",
        "\n",
        "# New context and question\n",
        "context_question = \"Mount Everest is the highest mountain in the world. question: 'What is the highest mountain in the world?'\"\n",
        "\n",
        "# Combine examples and prompt\n",
        "combined_qa_prompt = \"\\n\\n\".join(few_shot_examples + [context_question])\n",
        "\n",
        "# Generate inputs\n",
        "question_inputs = tokenizer(combined_qa_prompt, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
        "\n",
        "# Generate outputs\n",
        "question_outputs = model.generate(question_inputs[\"input_ids\"], max_length=50, num_beams=5, early_stopping=True)\n",
        "\n",
        "# Decode and print the answer\n",
        "print(tokenizer.decode(question_outputs[0], skip_special_tokens=True))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}